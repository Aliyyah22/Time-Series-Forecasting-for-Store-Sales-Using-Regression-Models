{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Store Sales Using Time Series Forecasting</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "### Introduction\n",
    "Accurate sales forecasting is essential for maximising inventory, resource allocation, and decision-making processes in the dynamic world of retail. In this project, we delve into the realm of time series forecasting for store sales, aiming to provide a robust predictive model for Favorita, a leading Ecuadorian-based grocery retailer. By harnessing the power of data and advanced analytics, we seek to enhance Favorita's operations, improve customer satisfaction, and drive business growth.\n",
    "\n",
    "### Business Objective\n",
    "The main objective of this project is to develop a robust and accurate time series forecasting model that predicts store sales for a wide range of products across Favorita stores. By leveraging historical sales data and relevant supplementary information, the model aims to provide reliable forecasts that enable Favorita to optimize its inventory management, resource allocation, and marketing strategies. The successful implementation of this model will contribute to improved operational efficiency, enhanced decision-making, and increased profitability for the retailer.\n",
    "\n",
    "### Business Goals\n",
    "The key business goals of this project include:\n",
    "\n",
    "- Improved Inventory Management: Accurate sales predictions will enable Favorita to manage inventory levels efficiently. \n",
    "\n",
    "- Enhanced Resource Allocation: With precise sales forecasts, Favorita can allocate human resources and logistics more effectively, ensuring that stores have adequate staff and supplies to meet customer demand.\n",
    "\n",
    "- Marketing and Promotion Strategies: By understanding the impact of promotions on sales, Favorita can tailor its marketing strategies to boost sales during specific periods. \n",
    "\n",
    "- Optimized Financial Planning: Accurate sales predictions facilitate better financial planning and budgeting.\n",
    "\n",
    "\n",
    "### Data Reqirements\n",
    "To successfully achieve the objectives of this project and build an accurate time series forecasting model for store sales, the following data is required:\n",
    "\n",
    "1.  Historical Sales Data\n",
    "        \n",
    "2.  Transaction Data\n",
    "        \n",
    "\n",
    "3.  Store Metadata \n",
    "       \n",
    "\n",
    "4.  Oil Price Data \n",
    "        \n",
    "\n",
    "5.  Holidays and Events Data \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis:\n",
    "\n",
    "- Null Hypothesis (H0): The intensity of promotion (onpromotion) does not have a significant impact on the average sales of products.\n",
    "\n",
    "- Alternative Hypothesis (H1): The intensity of promotion (onpromotion) has a significant impact on the average sales of products.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1.\tWhich dates have the lowest and highest sales for each year?\n",
    "\n",
    "2.\tWhich stores are among the top 10 in terms of total sales?\n",
    "\n",
    "3.\tDid the sales data show any noticeable changes in sales patterns around the time of the 2016 earthquake?\n",
    "\n",
    "4.\tWhich product families were the most frequently purchased (Top 5)?\n",
    "\n",
    "5.\tIs there a relationship between transactions and sales?\n",
    "\n",
    "6.\tIs there any association between oil prices and sales?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install required packages\n",
    "\n",
    "#Libraries for sql\n",
    "import pyodbc \n",
    "from dotenv import dotenv_values #import the dotenv_values function from the dotenv package\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#libraries for handling data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "##data visualizations\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import plotly.express as px\n",
    "\n",
    "##stat models \n",
    "from statsmodels.tsa.stattools import adfuller  ##for adf test\n",
    "from statsmodels.graphics.tsaplots import plot_acf \n",
    "from statsmodels.graphics.tsaplots import plot_pacf \n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import grangercausalitytests##grenger causality test\n",
    "\n",
    "\n",
    "##Error evaluations\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error,mean_squared_log_error\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "## Algorithms\n",
    "#from pmdarima import auto_arima\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Feature processing libraries\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#from category_encoders.binary import BinaryEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a connection by accessing connection string with defined environment variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load environment variables from .env file into a dictionary\n",
    "environment_variables = dotenv_values('.env')\n",
    "\n",
    "\n",
    "# Get the values for the credentials you set in the '.env' file\n",
    "database = environment_variables.get(\"DATABASE\")\n",
    "server = environment_variables.get(\"SERVER\")\n",
    "username = environment_variables.get(\"USERNAME\")\n",
    "password = environment_variables.get(\"PASSWORD\")\n",
    "\n",
    "connection_string = f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the connect method of the pyodbc library and pass in the connection string.\n",
    "connection = pyodbc.connect(connection_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the sql query to get the data is what what you see below. \n",
    "query =\"Select * from dbo.holidays_events\"\n",
    "query1=\"Select * from dbo.oil\"\n",
    "query2 =\"Select * from dbo.stores\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "### Data Collection\n",
    "Data for this project will be collected from 3 places, a database, OneDrive and GitHub\n",
    "\n",
    "### Data Description\n",
    "The training data includes dates, store, and product information, whether that item was being promoted, as well as the sales numbers. Additional files include supplementary information that may be useful in building your models\n",
    "\n",
    "**File Descriptions and Data Field Information**\n",
    "\n",
    "train.csv\n",
    "\n",
    "- The training data, comprising time series of features store_nbr, family, and onpromotion as well as the target sales.\n",
    "\n",
    "- **store_nbr** identifies the store at which the products are sold.\n",
    "\n",
    "- **family** identifies the type of product sold.\n",
    "\n",
    "- **sales** gives the total sales for a product family at a particular store at a given date. Fractional values are possible since products can be sold in fractional units (1.5 kg of cheese, for instance, as opposed to 1 bag of chips).\n",
    "\n",
    "- **onpromotion** gives the total number of items in a product family that were being promoted at a store at a given date.\n",
    "\n",
    "test.csv\n",
    "\n",
    "- The test data, having the same features as the training data. You will predict the target sales for the dates in this file.\n",
    "\n",
    "- The dates in the test data are for the 15 days after the last date in the training data.\n",
    "\n",
    "transaction.csv\n",
    "\n",
    "- Contains date, store_nbr and transaction made on that specific date.\n",
    "\n",
    "sample_submission.csv\n",
    "\n",
    "- A sample submission file in the correct format.\n",
    "\n",
    "stores.csv\n",
    "\n",
    "- Store metadata, including city,state, type, and cluster.\n",
    "\n",
    "- cluster is a grouping of similar stores.\n",
    "\n",
    "oil.csv\n",
    "\n",
    "- **Daily oil price** which includes values during both the train and test data timeframes. (Ecuador is an oil-dependent country and its economical health is highly vulnerable to shocks in oil prices.)\n",
    "\n",
    "holidays_events.csv\n",
    "\n",
    "- Holidays and Events, with metadata\n",
    "\n",
    "Additional holidays are days added, a regular calendar holiday, for example, as typically happens around Christmas (making Christmas Eve a holiday).\n",
    "\n",
    "Additional Notes\n",
    "\n",
    "- Wages in the public sector are paid every two weeks on the 15th and on the last day of the month. Supermarket sales could be affected by this.\n",
    "\n",
    "- A magnitude 7.8 earthquake struck Ecuador on April 16, 2016. People rallied in relief efforts donating water and other first need products which greatly affected supermarket sales for several weeks after the earthquake.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1 Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data from database\n",
    "holidays_events= pd.read_sql(query, connection)\n",
    "oil= pd.read_sql(query1, connection)\n",
    "stores= pd.read_sql(query2, connection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading csv data with pandas\n",
    "train = pd.read_csv(r\"C:\\Users\\eMARS COMPUTERS\\Desktop\\raheemah\\train.csv\")\n",
    "test = pd.read_csv(r\"C:\\Users\\eMARS COMPUTERS\\Desktop\\raheemah\\test.csv\")\n",
    "trans = pd.read_csv(r\"C:\\Users\\eMARS COMPUTERS\\Desktop\\raheemah\\transactions.csv\")#transactions\n",
    "sample_sub = pd.read_csv(r\"C:\\Users\\eMARS COMPUTERS\\Desktop\\raheemah\\sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>onpromotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000888</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3000889</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>BABY CARE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3000890</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>BEAUTY</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3000891</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>BEVERAGES</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000892</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>BOOKS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id        date  store_nbr      family  onpromotion\n",
       "0  3000888  2017-08-16          1  AUTOMOTIVE            0\n",
       "1  3000889  2017-08-16          1   BABY CARE            0\n",
       "2  3000890  2017-08-16          1      BEAUTY            2\n",
       "3  3000891  2017-08-16          1   BEVERAGES           20\n",
       "4  3000892  2017-08-16          1       BOOKS            0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view the csv datasets\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>sales</th>\n",
       "      <th>onpromotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BABY CARE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BEAUTY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BEVERAGES</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BOOKS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id        date  store_nbr      family  sales  onpromotion\n",
       "0   0  2013-01-01          1  AUTOMOTIVE    0.0            0\n",
       "1   1  2013-01-01          1   BABY CARE    0.0            0\n",
       "2   2  2013-01-01          1      BEAUTY    0.0            0\n",
       "3   3  2013-01-01          1   BEVERAGES    0.0            0\n",
       "4   4  2013-01-01          1       BOOKS    0.0            0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>transactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>2111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2</td>\n",
       "      <td>2358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>3</td>\n",
       "      <td>3487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>4</td>\n",
       "      <td>1922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  store_nbr  transactions\n",
       "0  2013-01-01         25           770\n",
       "1  2013-01-02          1          2111\n",
       "2  2013-01-02          2          2358\n",
       "3  2013-01-02          3          3487\n",
       "4  2013-01-02          4          1922"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>dcoilwtico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>93.139999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>92.970001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>93.120003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>93.199997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  dcoilwtico\n",
       "0  2013-01-01         NaN\n",
       "1  2013-01-02   93.139999\n",
       "2  2013-01-03   92.970001\n",
       "3  2013-01-04   93.120003\n",
       "4  2013-01-07   93.199997"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view the datasets in the database\n",
    "oil.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>type</th>\n",
       "      <th>locale</th>\n",
       "      <th>locale_name</th>\n",
       "      <th>description</th>\n",
       "      <th>transferred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-03-02</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Local</td>\n",
       "      <td>Manta</td>\n",
       "      <td>Fundacion de Manta</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-04-01</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Regional</td>\n",
       "      <td>Cotopaxi</td>\n",
       "      <td>Provincializacion de Cotopaxi</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-04-12</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Local</td>\n",
       "      <td>Cuenca</td>\n",
       "      <td>Fundacion de Cuenca</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-04-14</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Local</td>\n",
       "      <td>Libertad</td>\n",
       "      <td>Cantonizacion de Libertad</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-04-21</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Local</td>\n",
       "      <td>Riobamba</td>\n",
       "      <td>Cantonizacion de Riobamba</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date     type    locale locale_name                    description  \\\n",
       "0  2012-03-02  Holiday     Local       Manta             Fundacion de Manta   \n",
       "1  2012-04-01  Holiday  Regional    Cotopaxi  Provincializacion de Cotopaxi   \n",
       "2  2012-04-12  Holiday     Local      Cuenca            Fundacion de Cuenca   \n",
       "3  2012-04-14  Holiday     Local    Libertad      Cantonizacion de Libertad   \n",
       "4  2012-04-21  Holiday     Local    Riobamba      Cantonizacion de Riobamba   \n",
       "\n",
       "   transferred  \n",
       "0        False  \n",
       "1        False  \n",
       "2        False  \n",
       "3        False  \n",
       "4        False  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holidays_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>type</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Santo Domingo</td>\n",
       "      <td>Santo Domingo de los Tsachilas</td>\n",
       "      <td>D</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store_nbr           city                           state type  cluster\n",
       "0          1          Quito                       Pichincha    D       13\n",
       "1          2          Quito                       Pichincha    D       13\n",
       "2          3          Quito                       Pichincha    D        8\n",
       "3          4          Quito                       Pichincha    D        9\n",
       "4          5  Santo Domingo  Santo Domingo de los Tsachilas    D        4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - We will explore each dataset individually \n",
    " - We will answer each questions\n",
    " - A heavy focus will be on the train dataset since that's the most crucial dataset for our predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Datasets Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Checking for Missing values and an Overview of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## view information on train data\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of missing values in train\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### info of transcation data set\n",
    "trans.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view number of missing values in transcation data\n",
    "trans.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check info in oil data\n",
    "oil.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check missing values in oil data\n",
    "oil.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check informatiom in holiday data\n",
    "holidays_events.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check missing values in holiday data\n",
    "holidays_events.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view info in stores data\n",
    "stores.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view missing values in stores data\n",
    "stores.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes from the .info()\n",
    "\n",
    "- Every attribute of each dataset has the right datatype\n",
    "- All the datasets have no missing value except the oil dataset that has 43 missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Exploring the stores dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will investigate each column to extract valuable insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploring cluster column\n",
    "plt.figure(figsize=(10, 6))  \n",
    "sns.countplot(data=stores, x=\"cluster\")\n",
    "plt.title(f'Count of {\"cluster\"}')\n",
    "plt.xlabel(\"cluster\")\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 17 unique clusters with cluster 3 having the highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploring state column\n",
    "stores.state.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a graph\n",
    "state_counts = stores['state'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=state_counts.index, y=state_counts.values, palette='coolwarm')\n",
    "plt.title('Number of Stores in Each State')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 16 unique states with Pichincha having the highest number of stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##exploring city column\n",
    "city_counts = stores['city'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 7))  \n",
    "sns.barplot(x=city_counts.values, y=city_counts.index, palette='Dark2')\n",
    "plt.title('Number of Stores in Each City')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('City')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 22 unique cities with Quito having the highest count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploring type column\n",
    "type_counts = stores['type'].value_counts()\n",
    "\n",
    "sns.set(rc={\"figure.figsize\": (10, 4)})\n",
    "sns.barplot(x=type_counts.index, y=type_counts.values, palette=\"Dark2\")\n",
    "plt.title(\"Count of Store Types\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Type\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5 unique store types with stores type D having the highest count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploring store_nbr\n",
    "unique_values = len(stores['store_nbr'].unique())\n",
    "print(\"Number of unique values in 'store_nbr':\", unique_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Summary:\n",
    "- There are 17 unique clusters with cluster 3 having the highest count\n",
    "- There are 16 unique states\n",
    "- There are 22 unique cities with Quito having the highest count\n",
    "- There are 54 unique stores Across 16 states and 22 cities\n",
    "- There are 5 unique store types with stores type D having the highest count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Exploring the Transaction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will convert the date to date time\n",
    "trans['date'] = pd.to_datetime(trans['date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##make a copy of the transcation data incase a mistake is made\n",
    "#this copy will be used for the exploration\n",
    "trans_c=trans.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##checking range of dates\n",
    "trans_c[\"date\"].min(),trans_c[\"date\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##the date will be set as an index for analysis\n",
    "trans_c=trans_c.set_index(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view transaction column with date as index\n",
    "trans_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting our transaction Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot our transcation time series for insights\n",
    "fig = px.line(trans_c, x=trans_c.index, y='transactions', title='Transaction Time Series')\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##using seaborn so the visual appears on github\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=trans_c, x=trans_c.index, y='transactions')\n",
    "plt.title('Time Series with Seaborn')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Transactions')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes \n",
    "\n",
    "- We can see some spikes in transactions at the begining of each year it could be due to seasonal pattern in the data. We will resample for further investigations and better understand the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for outliers in transactions data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for outliers\n",
    "trans.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes after Checking for outliers:\n",
    "\n",
    "- There are outliers therefore, when plotting our resampled values, we will use median values for our analysis instead of the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling transcation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we will resample by months with median\n",
    "month_trans=trans_c.drop(labels= \"store_nbr\",axis= 1).resample(\"M\").median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view monthly transcations\n",
    "month_trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot our transcation time series for insights\n",
    "fig = px.line(month_trans, x=month_trans.index, y='transactions', title='Monthly transcations')\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using this visual so it appears on github\n",
    "#Plotting the data\n",
    "ax = month_trans.plot()\n",
    "\n",
    "# Adding a title to the plot\n",
    "plt.title('Monthly Transactions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "* The transaction dataset exhibits a recurring pattern of seasonality, with a noticeable spike in transactions occurring on the 31st of December every year.\n",
    "* Additionally, a minor increase in transactions is observed on the 31st of May in each year.\n",
    "* In 2016, there is a noticeable but relatively modest decrease in transaction activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## explore store_nbr column\n",
    "trans_c[\"store_nbr\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view top 10 stores with highest transactions\n",
    "store_tran= trans_c.groupby(\"store_nbr\")[\"transactions\"].agg(\"sum\").sort_values(ascending= False).head(10)\n",
    "\n",
    "sns.barplot(x=store_tran.index, y=store_tran.values)\n",
    "\n",
    "\n",
    "plt.title('Top 10 Store with the Highest Transactions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Transaction Dataset:\n",
    "- The store dataset had some seasonality with a spike in sales on every 23rd of December\n",
    "- store number 44 had the highest number of transactions\n",
    "- There are some outliers in the transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 Exploring the Holidays Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will convert the date to date time\n",
    "holidays_events['date'] = pd.to_datetime(holidays_events['date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##explore holiday type column\n",
    "###make a copy to not lose data mistakenly\n",
    "holi_c=holidays_events.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##checking range of dates\n",
    "holi_c[\"date\"].min(),holi_c[\"date\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holi_c[\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the value counts of the \"type\" column\n",
    "type_counts = holi_c[\"type\"].value_counts()\n",
    "\n",
    "# Create a bar plot using Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=type_counts.index, y=type_counts.values, palette=\"Dark2\")\n",
    "plt.title('Counts of Holiday Types')\n",
    "plt.xlabel('Holiday Type')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore locale column\n",
    "\n",
    "##Let's see which locale has the highest number of holidays\n",
    "plt.figure(figsize=(10, 6))  \n",
    "sns.countplot(data=holi_c, x=\"locale\")\n",
    "plt.title(f'Count of {\"locale\"}')\n",
    "plt.xlabel(\"locale\")\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##explore transferred column\n",
    "holi_c[\"transferred\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the value counts of the \"transferred\" column\n",
    "transferred_counts = holi_c[\"transferred\"].value_counts()\n",
    "\n",
    "# Create a pie chart using matplotlib\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(transferred_counts, labels=transferred_counts.index,  colors=['#1f77b4', '#ff7f0e'], autopct='%1.1f%%')\n",
    "plt.title('Distribution of \"transferred\" Column')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of the Holiday Column:\n",
    "\n",
    "- The majority of holidays were classified as national holidays.\n",
    "- The majority of holidays were celebrated on the same day they occurred.\n",
    "- With the exception of 12 holidays, most of the holidays were not marked as transferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Oil Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will convert the date to date time\n",
    "oil['date'] = pd.to_datetime(oil['date'], format='%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##checking range of dates\n",
    "oil[\"date\"].min(),oil[\"date\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a copy\n",
    "oil_c=oil.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##set date as index  \n",
    "oil_c= oil_c.set_index(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view oil\n",
    "oil_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our data overview,we noticed that the dcoilwtico column in the oil dataset has missing values.Since the first value is missing it will good to use backward fill to fill the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_c.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##using backward fil\n",
    "oil_c[\"dcoilwtico\"].fillna(method= \"bfill\",inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##let view oil dataset after filling the values\n",
    "oil_c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check number of mising values\n",
    "oil_c.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot our oil time series for insights\n",
    "fig = px.line(oil_c, x=oil_c.index, y='dcoilwtico',title='Oil Time Series')\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##using matplotlib visual appears on github\n",
    "ax=oil_c.plot()\n",
    "\n",
    "# Adding a title to the plot\n",
    "plt.title('Oil Time Series')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of the oil  dataset:\n",
    "- There was a price drop from 2014 to 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will convert the date to date time\n",
    "train['date'] = pd.to_datetime(train['date'], format='%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##checking range of dates\n",
    "train[\"date\"].min(),train[\"date\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a copy to ensure we can retrive the orginal dataframe incase of an error during our analysis\n",
    "train_c=train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set date as index\n",
    "train_c=train_c.set_index(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view train data\n",
    "train_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Checking the distribution of the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_c.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_c.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "* Sales column contain some outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## exploring family column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_c[\"family\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "The uniform count across all families is logical since even on days with no purchases, the product's family is still represented in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## exploring on promotion column\n",
    "ax=train_c[\"onpromotion\"].plot()\n",
    "\n",
    "# Adding a title to the plot\n",
    "plt.title('Time Series On Onpromtion')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain more insights,we will resample by day and months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling Onpromotion By Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##resample on promotion by day\n",
    "daily_promo=train_c[\"onpromotion\"].resample(\"D\").mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot our  time series for insights\n",
    "fig = px.line(daily_promo, x=daily_promo.index, y='onpromotion',title=(\"Daily Promotions\"))\n",
    "\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using matplotlib to visualize so it appears on github\n",
    "ax=daily_promo.plot()\n",
    "\n",
    "\n",
    "# Adding a title to the plot\n",
    "plt.title('Daily Promotions')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- From 2013 to 2014, no promotions were recorded.\n",
    "- Promotional activities showed an increase in both 2016 and 2017.\n",
    "- There was a noticeable decline in promotions at the start of each year between 2015 and 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling Onpromotion By Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Let's see what happened each month\n",
    "month_promo=train_c[[\"onpromotion\"]].resample(\"M\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time series after resampling by month\n",
    "fig = px.line(month_promo, x=month_promo.index, y='onpromotion',title=(\"Monthly Promotions\"))\n",
    "\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using matplotlib to visualize so it appears on github\n",
    "ax=month_promo.plot()\n",
    "\n",
    "# Adding a title to the plot\n",
    "plt.title('Monthly Promotions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes of promotion after resmapling on promotion by month:\n",
    "- Promotions experienced a reduction from January to May in 2015.\n",
    "- A significant surge in promotions occurred from April 30th in 2016, possibly attributed to the earthquake event.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the Sales Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_c.sales.plot()\n",
    "plt.title(\"Sales with respect to Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the apparent variability in the graph displayed above, I intend to perform resampling on various time scales, including daily, weekly and  monthly intervals. This will allow me to analyze how the sales pattern evolves over different time periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling Sales By Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##daily resampling\n",
    "sale_daily=train_c[\"sales\"].resample(\"D\").mean()\n",
    "\n",
    "plt.title(\"Daily Sales\")\n",
    "\n",
    "sale_daily.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- A noticeable overall upward trend is evident.\n",
    "- Sales experience a decline at the start of each year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling Days By Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weekly resampling\n",
    "sale_weekly=train_c[\"sales\"].resample(\"W\").mean()\n",
    "\n",
    "plt.title(\"Weekly Sales\")\n",
    "\n",
    "sale_weekly.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "The trend becomes evident when resampled weekly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling Sales By Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_monthly=train_c[\"sales\"].resample(\"M\").mean()\n",
    "\n",
    "plt.title(\"Monthly Sales\")\n",
    "\n",
    "sale_monthly.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trend can also be seen when resampled monthly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude the 'family' column\n",
    "corr_matrix = train_c.drop(columns=['family']).corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap (Excluding Family)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging The Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only be considering:\n",
    " \n",
    "- transaction\n",
    "- oil\n",
    "- train\n",
    "- holidays\n",
    " \n",
    " \n",
    "Also, since we have our target(sales) in the transaction column, we are going to filter all other datasets (from 2013-01-01 to 2017-08-15) to match the date of our train dataset. This approach will enable us to assess the impact of various variables on our sales within the same timeframe.\n",
    "\n",
    "Additionally, we will exclude the 'store' dataset from our analysis. Our objective is not to predict sales for individual products across specific stores. Instead, we aim to forecast unit sales across all stores within the Favorita."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out dates that are only in our Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes\n",
    "merge_data = pd.merge(train, trans, how='outer', on=['date', 'store_nbr'])\n",
    "merge_data1 = pd.merge(merge_data, oil, how='outer', on='date')\n",
    "merge_data2= pd.merge(merge_data1, holidays_events, how='outer', on='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the merged dataframe to the desired date range\n",
    "start_date = '2013-01-01'\n",
    "end_date = '2017-08-15'\n",
    "merge_data3 = merge_data2[merge_data2['date'].between(start_date, end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view merged data\n",
    "merge_data3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the merged data to answer questions\n",
    "merged_data_copy = merge_data3.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis Testing using T-Test**\n",
    "\n",
    "A **t-test** is a statistical hypothesis test used to determine if there is a significant difference between the means of two groups. It helps you assess whether the observed differences between the groups' sample means are likely to have occurred due to random chance or if they are statistically significant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis: Promotion Intensity\n",
    "\n",
    "**Null Hypothesis (H0):** The intensity of promotion (onpromotion) does not have a significant impact on the average sales of products.\n",
    "\n",
    "**Alternative Hypothesis (H1):** The intensity of promotion (onpromotion) has a significant impact on the average sales of products.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split onpromotion variable into promoted products and non-promoted products\n",
    "promoted_data = merged_data_copy[merged_data_copy['onpromotion'] >= 1]\n",
    "non_promoted_data = merged_data_copy[merged_data_copy['onpromotion'] < 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate average sales for promoted and non-promoted products\n",
    "average_sales_promoted = promoted_data['sales'].mean()\n",
    "average_sales_non_promoted = non_promoted_data['sales'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a t-test\n",
    "t_statistic, p_value = stats.ttest_ind(promoted_data['sales'], non_promoted_data['sales'], equal_var=False)\n",
    "\n",
    "# Determine the significance level (alpha)\n",
    "alpha = 0.05\n",
    "\n",
    "# Check if the p-value is less than the significance level\n",
    "if p_value < alpha:\n",
    "    result = \"Reject the null hypothesis\"\n",
    "else:\n",
    "    result = \"Fail to reject the null hypothesis\"\n",
    "\n",
    "# Print the results with p-value formatted to three decimal places\n",
    "print(\"Average sales for promoted products:\", average_sales_promoted)\n",
    "print(\"Average sales for non-promoted products:\", average_sales_non_promoted)\n",
    "print(\"T-statistic:\", t_statistic)\n",
    "print(f\"P-value: {p_value:.3f}\")  # Format p-value to three decimal places\n",
    "print(\"Conclusion:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average sales for promoted and non-promoted products\n",
    "average_sales_promoted = promoted_data['sales'].mean()\n",
    "average_sales_non_promoted = non_promoted_data['sales'].mean()\n",
    "\n",
    "# Create a bar plot using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=['Promoted', 'Non-Promoted'], y=[average_sales_promoted, average_sales_non_promoted])\n",
    "plt.xlabel('Promotion Status')\n",
    "plt.ylabel('Average Sales')\n",
    "plt.title('Average Sales for Promoted and Non-Promoted Products')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The average sales for promoted products is approximately 1139.83.\n",
    "- The average sales for non-promoted products is approximately 157.81.\n",
    "- The calculated t-statistic is approximately 395.83.\n",
    "- The calculated p-value is 0.000\n",
    "\n",
    "The average sales for promoted products is higher than the average sales for non-promoted products\n",
    "\n",
    "Based on the p-value being significantly lower than the significance level of 0.05, we can reject the null hypothesis. This suggests that there is a significant difference between the average sales of promoted products and non-promoted products. In other words, the presence of promotions has a statistically significant positive effect on the sales of products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Which dates have the lowest and highest sales for each year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year from the 'date' column\n",
    "merged_data_copy['year'] = merged_data_copy['date'].dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by year and calculate min and max sales\n",
    "yearly_sales_stats = merged_data_copy.groupby('year')['sales'].agg([('min_sales', 'min'), ('max_sales', 'max')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the dates with lowest and highest sales for each year\n",
    "lowest_sales_dates = merged_data_copy.loc[merged_data_copy.groupby('year')['sales'].idxmin(), ['year', 'date', 'sales']]\n",
    "highest_sales_dates = merged_data_copy.loc[merged_data_copy.groupby('year')['sales'].idxmax(), ['year', 'date', 'sales']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n",
    "print(\"\\nDates with Lowest Sales for Each Year:\")\n",
    "lowest_sales_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDates with Highest Sales for Each Year:\")\n",
    "highest_sales_dates\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dates with lowest sales for each year**\n",
    "\n",
    "This data indicates that on the dates mentioned, the sales were at their lowest, and the sales value recorded for those dates was 0.0 for all years (2013, 2014, 2015, 2016, and 2017). This could be due to various factors such as holidays, special events, or other circumstances that led to lower sales activity on those specific dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the given data\n",
    "data = {\n",
    "    'year': [2013, 2014, 2015, 2016, 2017],\n",
    "    'date': ['2013-11-12', '2014-12-08', '2015-12-14', '2016-05-02', '2017-04-02'],\n",
    "    'sales': [46271.0, 45361.0, 40351.46, 124717.0, 38422.625]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert 'date' column to datetime type\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Define a color palette\n",
    "colors = sns.color_palette(\"pastel\")\n",
    "\n",
    "# Create a bar plot using Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='year', y='sales', data=df, palette=colors)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Dates with Highest Sales for Each Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dates with highest sales for each year**\n",
    "\n",
    "- In the year 2013, the highest sales of 46,271.000 occurred on November 12, 2013.\n",
    "- In the year 2014, the highest sales of 45,361.000 occurred on December 8, 2014.\n",
    "- In the year 2015, the highest sales of 40,351.460 occurred on December 14, 2015.\n",
    "- In the year 2016, the highest sales of 124,717.000 occurred on May 2, 2016.\n",
    "- In the year 2017, the highest sales of 38,422.625 occurred on April 2, 2017.\n",
    "\n",
    "These dates represent the days with the highest sales values in each respective year. This information can be valuable for understanding trends in sales performance and identifying potentially impactful events or promotions that led to these high sales days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Which stores are among the top 10 in terms of total sales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by store number and calculate total sales for each store\n",
    "store_sales = train.groupby('store_nbr')['sales'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort stores by total sales in descending order and select the top 10\n",
    "top_10_stores = store_sales.sort_values(ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the top 10 stores and their total sales\n",
    "print(\"Top 10 Stores by Total Sales:\")\n",
    "print(top_10_stores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot using Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_10_stores.index, y=top_10_stores.values, palette=\"viridis\")\n",
    "plt.title('Top 10 Stores by Total Sales')\n",
    "plt.xlabel('Store Number')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.xticks(rotation=45)\n",
    "plt.gca().get_yaxis().get_major_formatter().set_scientific(False)  # Prevent scientific notation\n",
    "plt.gca().get_yaxis().get_major_formatter().set_useOffset(False)  # Prevent offset notation\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The store with store number 44 has at the highest total sales value of approximately 62,087,550, followed by the store with store number 45 of a total sales value of approximately 54,498,010. The lowest total sales is store number 50.  \n",
    "This information gives us insight into the stores that have generated the highest total sales, helping us understand the distribution of sales among the top-performing stores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Did the sales data show any noticeable changes in sales patterns around the time of the 2016 earthquake?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter sales data for the periods before and after the earthquake\n",
    "sales_before_earthquake = merged_data_copy[merged_data_copy['date'] < '2016-04-16']\n",
    "sales_after_earthquake = merged_data_copy[merged_data_copy['date'] >= '2016-04-16']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a line plot for sales trends before and after the earthquake\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sales_before_earthquake['date'], sales_before_earthquake['sales'], label='Before Earthquake')\n",
    "plt.plot(sales_after_earthquake['date'], sales_after_earthquake['sales'], label='After Earthquake')\n",
    "plt.axvline(x=pd.to_datetime('2016-04-16'), color='red', linestyle='--', label='Earthquake Date')\n",
    "plt.title('Sales Patterns Before and After 2016 Earthquake')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter data around earthquake date\n",
    "earthquake_date = pd.to_datetime('2016-04-16')\n",
    "window_days = 14  # Number of days before and after the earthquake\n",
    "start_date_before = earthquake_date - pd.DateOffset(days=window_days)\n",
    "end_date_before = earthquake_date\n",
    "start_date_after = earthquake_date\n",
    "end_date_after = earthquake_date + pd.DateOffset(days=window_days)\n",
    "\n",
    "data_before_earthquake = merged_data_copy[\n",
    "    (merged_data_copy['date'] >= start_date_before) & (merged_data_copy['date'] <= end_date_before)\n",
    "]\n",
    "data_before_earthquake['group'] = 'Before Earthquake'\n",
    "\n",
    "data_after_earthquake = merged_data_copy[\n",
    "    (merged_data_copy['date'] >= start_date_after) & (merged_data_copy['date'] <= end_date_after)\n",
    "]\n",
    "data_after_earthquake['group'] = 'After Earthquake'\n",
    "\n",
    "# Concatenate the dataframes\n",
    "combined_data = pd.concat([data_before_earthquake, data_after_earthquake])\n",
    "\n",
    "# Create Plotly figure\n",
    "fig = px.line(combined_data, x='date', y='sales', color='group', title='Sales Patterns Before and After 2016 Earthquake')\n",
    "\n",
    "# Add vertical line to indicate earthquake date\n",
    "fig.add_vline(x=earthquake_date, line_dash=\"dash\", line_color=\"red\", name=\"2016 Earthquake\")\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(xaxis_title='Date', yaxis_title='Sales', legend_title='Legend')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sales data showed some noticeable changes in sales patterns after the 2016 earthquake. Sales patterns changed significantly after the 2016 earthquake, with a noticeable spike in sales immediately following the event. This observation indicates that the earthquake might have had a significant impact on consumer behavior and purchasing patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** Which product families were the most frequently purchased (Top 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by family and calculate total sales for each family\n",
    "family_sales = train.groupby('family')['sales'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort families by total sales in descending order and select the top 5\n",
    "top_5_families = family_sales.sort_values(ascending=False).head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the top 5 families and their total sales\n",
    "print(\"Top 5 Families by Total Sales:\")\n",
    "print(top_5_families)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot using Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_5_families.index, y=top_5_families.values, palette=\"viridis\")\n",
    "plt.title('Top 5 Families by Total Sales')\n",
    "plt.xlabel('Product Family')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"GROCERY I\" family has the highest total sales, followed by \"BEVERAGES,\" \"PRODUCE,\" \"CLEANING,\" and \"DAIRY.\" These product families have significantly higher total sales compared to other families in the dataset. This information can be valuable for understanding customer preferences and optimizing product offerings in these top-performing categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** Is there a relationship between transactions and sales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation between transactions and sales\n",
    "correlation = merged_data_copy['transactions'].corr(merged_data_copy['sales'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the correlation value\n",
    "print(\"Correlation between Transactions and Sales:\", correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficient between transactions and sales is approximately 0.215. This indicates a positive correlation between these two variables. A positive correlation means that as the number of transactions increases, the sales tend to increase as well. However, the correlation coefficient of 0.215 suggests that the relationship between transactions and sales is not very strong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(merged_data_copy['transactions'], merged_data_copy['sales'], alpha=0.5)\n",
    "plt.title('Transactions vs. Sales')\n",
    "plt.xlabel('Transactions')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot also supports this interpretation. The points on the scatter plot show a general trend of increasing sales with increasing transactions, but there is also a significant amount of variability in the data points, indicating that other factors may be influencing sales as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** Is there any association between oil prices and sales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation between sales and oil prices\n",
    "correlation_sales_oil = merged_data_copy['sales'].corr(merged_data_copy['dcoilwtico'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the correlation coefficient\n",
    "print(\"Correlation between Sales and Oil Prices:\", correlation_sales_oil)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficient between sales and oil prices is approximately -0.079. This value indicates a weak negative correlation between the two variables. In other words, as oil prices increase or decrease, there is a very slight tendency for sales to decrease or increase, respectively. However, the correlation is quite close to zero, which suggests that there is no strong linear relationship between sales and oil prices in the given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot to visualize the correlation\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=merged_data_copy, x='dcoilwtico', y='sales')\n",
    "plt.title('Correlation between Sales and Oil Prices')\n",
    "plt.xlabel('Oil Prices (dcoilwtico)')\n",
    "plt.ylabel('Sales')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The points on the plot are scattered without a clear linear trend, indicating a weak relationship between these two variables. This observation is consistent with the correlation coefficient of approximately -0.08, which is close to zero. A correlation value close to zero suggests that there is little to no linear correlation between sales and oil prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After answering our questions,we will drop irrelevant columns in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dropping Irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop family,id,store_nbr,locale_name,description since we won't need them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasons For dropping Columns\n",
    "\n",
    "**Family Column**: We dropped the 'family' column because it  represents the product family or category to which each item belongs.We are primarily interested in sales forecasting or analysis at a higher level of aggregation, such as overall sales trends, therefore the individual product family might not be as relevant, and it can be dropped to simplify the dataset.\n",
    "\n",
    "**Id Column**: The 'id' column appears to be an identifier, which is typically unique for each row.It doesn't provide any meaningful information for our analysis, it can be safely dropped as it doesn't contribute to our sales analysis.\n",
    "\n",
    "**Store_nbr Column**:Our analysis is focused on overall sales trends and not store-specific trends therefore we might drop the 'store_nbr' column. This column  represents the store number, and removing it can help we analyze sales patterns across all stores collectively.\n",
    "\n",
    "**Locale_name and Description Columns**: Similar to 'family,' 'locale_name' and 'description' columns might contain descriptive information that is not essential for sales analysis.We don't plan to use these columns for specific analysis and they contain redundant information, so we can drop them for simplicity.\n",
    "\n",
    "By dropping less relevant columns, we can streamline our dataset and potentially improve the efficiency of your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we drop the columns\n",
    "final_merge=merge_data3.drop([\"family\",\"id\",\"store_nbr\",\"locale_name\",\"description\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view data after dropping\n",
    "final_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view missing values\n",
    "final_merge.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For transactions, we will replace the na values with 0 since it means no transaction was recorded on the said date\n",
    "\n",
    "# Create a SimpleImputer instance with the strategy set to 'constant' and fill_value set to 0\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "\n",
    "# Fit the imputer on your data and transform the column\n",
    "final_merge[\"transactions\"] = imputer.fit_transform(final_merge[[\"transactions\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for sales and onpromotion we will drop the missing values\n",
    "final_merge= final_merge.dropna(subset=[\"sales\", \"onpromotion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the holiday attributes, we'll fill in all the missing rows with non-holiday values. Similarly, for the \"transferred\" attribute, we'll replace the missing rows with \"false,\" as they were not transferred. As for the \"dcoilwtico\" attribute, since the values are closely aligned, we'll apply a backfill approach to handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merge[\"type\"]=final_merge[\"type\"].replace(np.nan, \"Not Holiday\")\n",
    "\n",
    "final_merge[\"locale\"]=final_merge[\"locale\"].replace(np.nan, \"Not Holiday\")\n",
    "\n",
    "final_merge[\"transferred\"]=final_merge[\"transferred\"].replace(np.nan, \"False\")\n",
    "\n",
    "final_merge[\"dcoilwtico\"]= final_merge[\"dcoilwtico\"].fillna(method= \"bfill\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##After filling the missing values we check to see if the changes were applied\n",
    "final_merge.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##let's rename the type column to holiday type\n",
    "final_merge= final_merge.rename(columns= {\"type\": \"holiday\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Let's view the dataset\n",
    "final_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final merge data will be used when modelling with traditional models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Modelling Using Statistical Time Series Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For modelling using statistical models,because they do not have the power to operate on large data set we will do a univariate analysis to be able to perform modelling with these statistical models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train dataset will be used for the statistical modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create univariate data for modelliing\n",
    "univar_sale= train_c.drop([\"id\", \"store_nbr\", \"family\", \"onpromotion\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## view univariate data\n",
    "univar_sale.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationarity Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we state a null and alternative hypothesis for test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Null Hypothesis**-Series possesses a unit root and hence is not stationary\n",
    "\n",
    "**Alternative Hypothesis**-Series is Staionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to forecast the general sales for Favorita,therefore we are interested in aggregating and analyzing the total sales for each day.\n",
    "Therefore **Augmented Dickey-Fuller (ADF)** test will be used to the stationarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by date and aggregating sales\n",
    "univar_sale= univar_sale.groupby(univar_sale.index).agg({\"sales\": sum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure using Plotly Express\n",
    "fig = px.line(univar_sale, x=univar_sale.index, y='sales', title='Time Series with Slider')\n",
    "\n",
    "# Add a slider for date selection\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##using matplotlib so visual appears on github\n",
    "univar_sale.plot()\n",
    "plt.title(\"Sales Over Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes From The Plot:\n",
    "\n",
    "- Detecting changes, such as trends and seasonality, in the data is challenging due to the sheer volume of data points.\n",
    "\n",
    "- Additionally, there are noticeable abrupt declines at the start of each year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Augmented Dickey-Fuller (ADF) test to perform stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the Augmented Dickey-Fuller (ADF) test\n",
    "result = adfuller(univar_sale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the ADF test results\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])\n",
    "print('Critical Values:') \n",
    "for key, value in result[4].items():\n",
    "    print(f'{key}: {value}')\n",
    "\n",
    "if result[1]>0.05:\n",
    "    print(\"Series is not stationary\")\n",
    "else:\n",
    "    print(\"Series is stationarity\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing ADF test,we check our p-value. If the p-value is below a certain threshold (0.05), it suggests that the series is likely stationary. If it's above the threshold, the series might be non-stationary.From our test our p vallue which is 0.089 is > than 0.05 which means our time series is not stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoregressive (AR) model is a type of time series model used in statistics for analyzing and forecasting time series data.autocorrelation and partial autocorrelation are important concepts in autoregressive (AR) modeling and time series analysis in general. They help in identifying and selecting the appropriate lag orders for an AR model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the accuracy of our forecasts, it's crucial to examine whether the current value in a time series has a connection with its previous values. This is precisely why I'm creating both autocorrelation and partial autocorrelation plots. These plots help us assess the extent to which the current value depends on its past values and allow us to identify any significant patterns or relationships in the data that can aid in forecasting.\n",
    "\n",
    "**Autocorrelation measures** the degree of similarity between a time series and a lagged version of itself. It helps you understand the underlying patterns, seasonality, and potential dependencies within your time series data.\n",
    "\n",
    "**Partial autocorrelation** is a statistical technique used to determine the correlation between a time series and its lagged values while controlling for the correlations at shorter lag intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using AutoCorrelation and Partial AutoCorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "# Plot the autocorrelation on the first subplot\n",
    "plot_acf(univar_sale,lags=30, ax=axes[0])\n",
    "axes[0].set_title('Autocorrelation Plot(ACF)')\n",
    "axes[0].set_xlabel('Lag')\n",
    "\n",
    "# Plot the partial autocorrelation on the second subplot\n",
    "plot_pacf(univar_sale,lags=30, ax=axes[1])\n",
    "axes[1].set_title('Partial Autocorrelation Plot(PACF)')\n",
    "axes[1].set_xlabel('Lag')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the combined plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes From ACF/PACF Chart: \n",
    "\n",
    "* When examining the Autocorrelation Function (ACF) chart, we observe that all lag values are situated above our significance threshold. Notably, the most prominent lag is observed at a lag of 7.\n",
    "\n",
    "* In the Partial Autocorrelation Function (PACF) chart, we notice that a majority of lag values are above the significance level. Particularly, we observe strong lags at positions 1 and 6.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 80% of our data for train and 20% for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we check the shape and split\n",
    "univar_sale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we split the data into train and test\n",
    "train_1=univar_sale[0:1347]\n",
    "test_1=univar_sale[1347:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view train\n",
    "train_1.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Autoregressive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoregressive model, the basic idea is to predict the future values of a time series variable based on its own past values. It assumes that the current value of the time series is a linear combination of its previous values, with some randomness or error term.\n",
    "AR models are often combined with other components like moving averages to create more comprehensive time series models like ARIMA and SARIMA, which can handle more complex time series patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit our AR model\n",
    "model= AutoReg(train_1, lags=10).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we use our model to predict\n",
    "AR_pred = model.predict(start=len(train_1), end=len(train_1) + len(test_1) - 1, dynamic=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the actual values (training and testing data)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_1.index, train_1['sales'], label='Training Data')\n",
    "plt.plot(test_1.index, test_1['sales'], label='Testing Data')\n",
    "\n",
    "# Plot the predictions\n",
    "plt.plot(test_1.index, AR_pred, label='Predictions')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend(loc='best')\n",
    "plt.title('AR Model Forecast')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate our model\n",
    "mse=mean_squared_error(test_1,AR_pred)\n",
    "msle=mean_squared_log_error(test_1,AR_pred)\n",
    "rmse=np.sqrt(mean_squared_error(test_1,AR_pred)).round(2)\n",
    "rmsle=np.sqrt(mean_squared_log_error(test_1,AR_pred)).round(2)\n",
    "\n",
    "results=pd.DataFrame([[\"AR\",mse,msle,rmse,rmsle]],columns=[\"Model\",\"MSE\",\"MSLE\",\"RMSE\",\"RMSLE\"])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ARIMA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA stands for Autoregressive Integrated Moving Average.ARIMA models are widely used for time series forecasting because they can capture a wide range of time series patterns, including trends and seasonality. They are a fundamental tool in the field of time series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform stepwise to get best model we will use for arima\n",
    "stepwise_fit=auto_arima(train_1[\"sales\"],trace=True,suppress_warnings=True)\n",
    "stepwise_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit our model\n",
    "model1=ARIMA(train_1,order=(5,1,5)).fit()\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use our model to predict\n",
    "ARIMA_pred=model1.predict(start=len(train_1), end=len(train_1) + len(test_1) - 1, typ=\"levels\")\n",
    "print(ARIMA_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the actual values (training and testing data)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_1.index, train_1, label='Training Data')\n",
    "plt.plot(test_1.index, test_1, label='Testing Data')\n",
    "\n",
    "# Plot the predictions\n",
    "plt.plot(test_1.index, ARIMA_pred, label='Predictions')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend(loc='best')\n",
    "plt.title('ARIMA Model Forecast')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate our model\n",
    "mse=mean_squared_error(test_1,ARIMA_pred)\n",
    "msle=mean_squared_log_error(test_1,ARIMA_pred)\n",
    "rmse=np.sqrt(mean_squared_error(test_1,ARIMA_pred)).round(2)\n",
    "rmsle=np.sqrt(mean_squared_log_error(test_1,ARIMA_pred)).round(2)\n",
    "\n",
    "results1=pd.DataFrame([[\"ARIMA\",mse,msle,rmse,rmsle]],columns=[\"Model\",\"MSE\",\"MSLE\",\"RMSE\",\"RMSLE\"])\n",
    "results1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SARIMA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SARIMA (Seasonal Autoregressive Integrated Moving Average) model is an extension of the ARIMA (Autoregressive Integrated Moving Average) model that incorporates seasonality into the analysis and forecasting of time series data. SARIMA models are used to model and predict time series data that exhibit both non-seasonal and seasonal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit our model\n",
    "model2=SARIMAX(train_1,order=(6,1,7),seasonal_order=(0,0,0,12)).fit()\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##we predict with our model\n",
    "SARIMA_pred=model2.predict(start=len(train_1), end=len(train_1) + len(test_1) - 1, typ=\"levels\")\n",
    "print(SARIMA_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the actual values (training and testing data)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_1.index, train_1, label='Training Data')\n",
    "plt.plot(test_1.index, test_1, label='Testing Data')\n",
    "\n",
    "# Plot the predictions\n",
    "plt.plot(test_1.index, SARIMA_pred, label='Predictions')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend(loc='best')\n",
    "plt.title('SARIMA Model Forecast')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate our model\n",
    "mse=mean_squared_error(test_1,SARIMA_pred)\n",
    "msle=mean_squared_log_error(test_1,SARIMA_pred)\n",
    "rmse=np.sqrt(mean_squared_error(test_1,SARIMA_pred)).round(2)\n",
    "rmsle=np.sqrt(mean_squared_log_error(test_1,SARIMA_pred)).round(2)\n",
    "\n",
    "results2=pd.DataFrame([[\"SARIMA\",mse,msle,rmse,rmsle]],columns=[\"Model\",\"MSE\",\"MSLE\",\"RMSE\",\"RMSLE\"])\n",
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the results DataFrames into a single table\n",
    "combined_results = pd.concat([results, results1, results2])\n",
    "\n",
    "# Reset the index\n",
    "combined_results = combined_results.reset_index(drop=True)\n",
    "\n",
    "# Sort the combined results table\n",
    "combined_results = combined_results.sort_values(by=\"RMSLE\")\n",
    "\n",
    "# Display the combined results table\n",
    "combined_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "After comparing our 3 statisticals models,SARIMA performed the best by having the lowest Root Mean Square Log Error(RMSLE) of 0.29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making our Series Stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use many methods to make our series stationary but we will decompose and perform box cox to make variance constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposing our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose the time series\n",
    "decomposed_data = sm.tsa.seasonal_decompose(univar_sale, model='multiplicative', period=365)\n",
    "\n",
    "# Plot the decomposed components\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(411)  # 4 rows, 1 column, subplot 1 (original data)\n",
    "plt.plot(univar_sale, label='Original Data')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(412)  # subplot 2 (trend component)\n",
    "plt.plot(decomposed_data.trend, label='Trend Component')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(413)  # subplot 3 (seasonal component)\n",
    "plt.plot(decomposed_data.seasonal, label='Seasonal Component')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(414)  # subplot 4 (residuals)\n",
    "plt.plot(decomposed_data.resid, label='Residuals')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box Cox Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing box cox\n",
    "data_boxcox=pd.Series(boxcox(univar_sale[\"sales\"],lmbda=0),index=univar_sale.index)\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(data_boxcox,label='Before Box Cox Transformation')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Before Box Cox Transformation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we difference to remove trend\n",
    "data_boxcox_diff=pd.Series(data_boxcox-data_boxcox.shift(),univar_sale.index)\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(data_boxcox_diff,label='After Box Cox Transformation And Differencing')\n",
    "plt.legend(loc='best')\n",
    "plt.title('After Box Cox Transformation And Differencing')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###check if data is now stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values, if any\n",
    "data_boxcox_diff = data_boxcox_diff.dropna()\n",
    "\n",
    "# Perform the Augmented Dickey-Fuller (ADF) test\n",
    "adf_test_result = adfuller(data_boxcox_diff)\n",
    "\n",
    "# Print the ADF test results\n",
    "print('ADF Statistic:', adf_test_result[0])\n",
    "print('p-value:', adf_test_result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in adf_test_result[4].items():\n",
    "    print(f'{key}: {value}')\n",
    "\n",
    "if adf_test_result[1] > 0.05:\n",
    "    print(\"Series is not stationary\")\n",
    "else:\n",
    "    print(\"Series is stationary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data exhibits stationarity because our p-value is less than the significance level (0.05) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Multivariate Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to analyze how the remaining columns influence or impact sales. This approach is expected to enhance the precision of our forecasting models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Feature Selection For Multivariate Time Series Forecasting\n",
    "- Conducting Initial Feature Selection with Granger's Causality Test\n",
    "- Afterwards we will perform some feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithms we will be using for our modelling include:\n",
    "\n",
    "- SARIMAX\n",
    "- Linear Regression\n",
    "- Decision Tree\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Feature Selection using Granger Causality Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Granger causality** test serves as a valuable metric in our quest for model accuracy. It allows us to examine whether one time series has a discernible impact on another time series. Keep these important points in mind:\n",
    "\n",
    "- The null hypothesis posits that y (the second variable) does not influence x (the first variable). Consequently, if the p-value exceeds 0.05, we will accept this hypothesis. Conversely, if the p-value falls below 0.05, we will reject it.\n",
    "\n",
    "- To ensure a comprehensive analysis, we will conduct the test across a span of 30 lags. This extensive examination will empower us to make well-informed decisions regarding causality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "The granger causality test works for numeric variables so we will assume our categorical variables have an effect on sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our dataset which is(final_merge) will be used here for the multivariate forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view dataset\n",
    "final_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since granger test uses only numeric columns we take our numeric columns\n",
    "\n",
    "# Select numeric columns excluding 'sales' from final_merge\n",
    "numeric_columns = final_merge.drop('sales', axis=1).select_dtypes(include=['number'])\n",
    "\n",
    "# Display the list of numeric columns excluding 'sales'\n",
    "print(numeric_columns.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing the test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the size of the dataset taking the columns you want to check the causality requires a lot of computational so to mitigate this we aggregate the columns and then we perform the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregate our data\n",
    "final_merge= final_merge.groupby([\"date\", \"holiday\",\"locale\", \"transferred\" ]).agg({\"sales\": \"sum\", \"onpromotion\": \"sum\",\"transactions\": \"sum\",\"dcoilwtico\": \"sum\" })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##view agrregated\n",
    "final_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset our index\n",
    "final_merge=final_merge.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set date as index\n",
    "final_merge= final_merge.set_index(\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the causality between onpromotion and sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the maximum lag for testing causality\n",
    "max_lag = 30 \n",
    "\n",
    "# Perform the Granger causality test\n",
    "test_result = grangercausalitytests(final_merge[['onpromotion','sales']], max_lag, verbose=False)\n",
    "\n",
    "# Print the test results\n",
    "for lag in range(1, max_lag + 1):\n",
    "    print(f'Lag {lag}:')\n",
    "    print('F-statistic:', test_result[lag][0]['ssr_ftest'][0])\n",
    "    print('p-value:', test_result[lag][0]['ssr_ftest'][1])\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights on the Causality Between Sales and On-Promotion:\n",
    "- Across 30 lags, our p-values consistently fall below the 0.05 threshold, leading us to reject the null hypothesis in favor of granger causality.\n",
    "\n",
    "- Furthermore, as our p-value is below 0.05, we affirm the primary hypothesis of our study, which is that on-promotion has a significant impact on sales.\n",
    "\n",
    "In simpler terms, on-promotion has a causal relationship with sales, confirming its importance for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the causality between transactions and sales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the maximum lag for testing causality\n",
    "max_lag = 30 \n",
    "\n",
    "# Perform the Granger causality test\n",
    "test_result = grangercausalitytests(final_merge[['transactions','sales']], max_lag, verbose=False)\n",
    "\n",
    "# Print the test results\n",
    "for lag in range(1, max_lag + 1):\n",
    "    print(f'Lag {lag}:')\n",
    "    print('F-statistic:', test_result[lag][0]['ssr_ftest'][0])\n",
    "    print('p-value:', test_result[lag][0]['ssr_ftest'][1])\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes on the causality between sales and transaction:\n",
    "\n",
    "- Over 30 lags, our p-value seem to be less than 0.05;therefore, we reject the null hypothesis.\n",
    "\n",
    "Therefore we can transactions has a causal effect on sales;therefore, we keep it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the causality between dcoilwtico and sales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the maximum lag for testing causality\n",
    "max_lag = 30 \n",
    "\n",
    "# Perform the Granger causality test\n",
    "test_result = grangercausalitytests(final_merge[['dcoilwtico','sales']], max_lag, verbose=False)\n",
    "\n",
    "# Print the test results\n",
    "for lag in range(1, max_lag + 1):\n",
    "    print(f'Lag {lag}:')\n",
    "    print('F-statistic:', test_result[lag][0]['ssr_ftest'][0])\n",
    "    print('p-value:', test_result[lag][0]['ssr_ftest'][1])\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Notes on the causality between sales and dcoilwtico:\n",
    "\n",
    "- Over 30 lags,  our p-value seem to be greater than 0.05 from the 20th lag;therefore, we accept the null hypothesis amd drop the column\n",
    "\n",
    "In basic terms, dcoilwtico has no causal effect on sales;therefore, we drop it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing the test,we concluded that dcoilwtico will be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##dropping dcoilwtico column\n",
    "final_merge=final_merge.drop(\"dcoilwtico\", axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### view dataset after dropping column\n",
    "final_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will undertake the following tasks:\n",
    "\n",
    "- Generate new features\n",
    "- Encode our categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting my dataset to use for my statistical multivariate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##view the shape\n",
    "final_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we split the data into train and test\n",
    "s_train=final_merge[0:1362]\n",
    "s_test=final_merge[1362:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Label Encoder: We'll use a Label Encoder for the \"transferred\" column since it contains binary values, specifically \"True\" and \"False.\"\n",
    "\n",
    "- Ordinal Encoder: For the \"locale\" column, which represents hierarchical categories like \"National\" and \"Local,\" we'll use an Ordinal Encoder to maintain the ordinal relationship between these categories.\n",
    "\n",
    "- Binary Encoder: Lastly, we'll apply a Binary Encoder to the \"holiday\" column, which includes various categorical variables. This technique will help transform these categorical values into binary code representations efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding the transferred column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we check the number of unique values in transferred column\n",
    "s_train[\"transferred\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-We can see the number of unique values in the transferred column are 3 instead of 2.This is because there is a presence of a false string.This will be replaced with the boolean version of the false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing false string with boolean false\n",
    "s_train[\"transferred\"]= s_train[\"transferred\"].replace(\"False\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we confirm changes\n",
    "s_train[\"transferred\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making changes to the transferred column,we create an instance of the label encoder we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating insatnce of encoder\n",
    "LE= LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use the label encoder to fit and transform the train\n",
    "s_train[\"transferred\"]=LE.fit_transform(s_train[\"transferred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we transform the test\n",
    "s_test[\"transferred\"]=LE.transform(s_test[\"transferred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##checking to confirm changes\n",
    "s_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Holiday Column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view unique values\n",
    "s_train[\"holiday\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A binary encoder will be used since there are various attributes in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##we give the encoder the column we want to encode and create an instance\n",
    "BE= BinaryEncoder(cols= \"holiday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we fit transform the train set\n",
    "s_train= BE.fit_transform(s_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we transform the test\n",
    "s_test= BE.transform(s_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##view changes\n",
    "s_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Encoding The Locale Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view unique values in locale column\n",
    "s_train[\"locale\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ordinal encoder is used for categorical variables with hierarchy and the variables in the locale column look like there is a form of hierarchy.Therefore i will create my order of hierarchy from highest to lowest and use the ordinal encoder\n",
    "\n",
    "My order will be National,Regional,Local,Not holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create a hierarchy\n",
    "hier=[\"National\", \"Regional\", \"Local\", \"Not Holiday\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OrdinalEncoder with the hierarchy\n",
    "OE= OrdinalEncoder(categories=[hier])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the training data (s_train)\n",
    "s_train[[\"locale\"]] = OE.fit_transform(s_train[[\"locale\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the testing data (s_test) using the same encoder\n",
    "s_test[[\"locale\"]] = OE.transform(s_test[[\"locale\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirm changes\n",
    "s_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##since we will be using the date to create new features, we will reset our index\n",
    "s_test=s_test.reset_index()\n",
    "s_train=s_train.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Statistical Models For Multivariate Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SARIMAX for Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the SARIMAX model with our data, we need to set up our endogenous (endo) and exogenous (exo) variables.\n",
    "\n",
    "**Endogenous variable** are the ones you are trying to model and predict based on its own past values and potentially, exogenous variables. It represents the primary variable of interest in your time series analysis.\n",
    "\n",
    "**Exogenous variables** are often used in time series models to improve the accuracy of forecasts. They can provide additional information and context that helps the model make better predictions.These are additional variables that can influence the variable of interest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set my date as index\n",
    "sari_test=s_test.set_index(\"date\")\n",
    "sari_train=s_train.set_index(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### getting my exogenous and endogenous features\n",
    "train_endog=sari_train[[\"sales\"]]#### endogenous train\n",
    "test_endog=sari_test[[\"sales\"]]### endogenous test\n",
    "\n",
    "train_exog=sari_train.drop(\"sales\", axis= 1)### exogenous train\n",
    "test_exog=sari_test.drop(\"sales\", axis= 1)### exogenous test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your SARIMAX model and fit the model\n",
    "model4= sm.tsa.SARIMAX(train_endog, exog=train_exog, order=(6, 1, 7), seasonal_order=(0, 0, 0, 12)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make forecasts for the test set\n",
    "SARIMAX_pred= model4.get_forecast(steps=len(test_endog), exog=test_exog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs. predicted values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_endog.index, test_endog['sales'], label='Actual')\n",
    "plt.plot(test_endog.index, SARIMAX_pred.predicted_mean, label='Predicted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Actual vs. Predicted Sales')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluate our model\n",
    "mse = mean_squared_error(test_endog, SARIMAX_pred.predicted_mean)\n",
    "msle = mean_squared_log_error(test_endog, SARIMAX_pred.predicted_mean)\n",
    "rmse = np.sqrt(mse).round(2)\n",
    "rmsle = np.sqrt(msle).round(2)\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results3 = pd.DataFrame([[\"SARIMAX\", mse, msle, rmse, rmsle]], columns=[\"Model\", \"MSE\", \"MSLE\", \"RMSE\", \"RMSLE\"])\n",
    "\n",
    "# Print the evaluation results\n",
    "results3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Modelling with Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load our dataframes again\n",
    "s_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering for Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add extra features to our dataset to help increase the accuracy in prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename s_train to m_train\n",
    "m_train = s_train\n",
    "\n",
    "# Rename s_test to m_test\n",
    "m_test = s_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the features to m_train\n",
    "m_train['year'] = m_train['date'].dt.year\n",
    "m_train['month'] = m_train['date'].dt.month\n",
    "m_train['day'] = m_train['date'].dt.day\n",
    "m_train['day_of_week'] = m_train['date'].dt.dayofweek\n",
    "m_train['day_of_year'] = m_train['date'].dt.dayofyear\n",
    "m_train['week_of_year'] = m_train['date'].dt.isocalendar().week\n",
    "m_train['quarter'] = m_train['date'].dt.quarter\n",
    "m_train['is_weekend'] = (m_train['date'].dt.dayofweek // 5 == 1).astype(int)\n",
    "m_train['day_of_month'] = m_train['date'].dt.day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the features to m_test\n",
    "m_test['year'] = m_test['date'].dt.year\n",
    "m_test['month'] = m_test['date'].dt.month\n",
    "m_test['day'] = m_test['date'].dt.day\n",
    "m_test['day_of_week'] = m_test['date'].dt.dayofweek\n",
    "m_test['day_of_year'] = m_test['date'].dt.dayofyear\n",
    "m_test['week_of_year'] = m_test['date'].dt.isocalendar().week\n",
    "m_test['quarter'] = m_test['date'].dt.quarter\n",
    "m_test['is_weekend'] = (m_test['date'].dt.dayofweek // 5 == 1).astype(int)\n",
    "m_test['day_of_month'] = m_test['date'].dt.day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original 'date' column\n",
    "m_train.drop(columns=['date'], inplace=True)\n",
    "m_test.drop(columns=['date'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view colums after making changes\n",
    "m_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##we get our features and labels\n",
    "Xm_train= m_train.drop(\"sales\", axis= 1)\n",
    "\n",
    "ym_train=m_train[\"sales\"]\n",
    "\n",
    "Xm_test=m_test.drop(\"sales\", axis= 1)\n",
    "\n",
    "ym_test=m_test[\"sales\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling Using Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an insatnce for the model\n",
    "lin_model=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##fit the data on the model\n",
    "model_lin=lin_model.fit(Xm_train,ym_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions\n",
    "lin_pred=model_lin.predict(Xm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ym_test, label='Actual')\n",
    "plt.plot(lin_pred, label='Predicted')\n",
    "plt.title('Linear Regression Predictions')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate our model\n",
    "mse=mean_squared_error(ym_test,lin_pred)\n",
    "msle=mean_squared_log_error(ym_test,lin_pred)\n",
    "rmse=np.sqrt(mean_squared_error(ym_test,lin_pred)).round(2)\n",
    "rmsle=np.sqrt(mean_squared_log_error(ym_test,lin_pred)).round(2)\n",
    "\n",
    "results4=pd.DataFrame([[\"Linear Regression\",mse,msle,rmse,rmsle]],columns=[\"Model\",\"MSE\",\"MSLE\",\"RMSE\",\"RMSLE\"])\n",
    "results4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling Using Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an instance for our model\n",
    "decision_tree=DecisionTreeRegressor(random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the data on our model\n",
    "model_dtree=decision_tree.fit(Xm_train,ym_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions\n",
    "tree_pred=model_dtree.predict(Xm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualise our actual and predicted values\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(ym_test,label=\"Actual sales\")\n",
    "plt.plot(tree_pred,label=\"Predicted\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Decision Tree Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate our model\n",
    "mse=mean_squared_error(ym_test,tree_pred)\n",
    "msle=mean_squared_log_error(ym_test,tree_pred)\n",
    "rmse=np.sqrt(mean_squared_error(ym_test,tree_pred)).round(2)\n",
    "rmsle=np.sqrt(mean_squared_log_error(ym_test,tree_pred)).round(2)\n",
    "\n",
    "results5=pd.DataFrame([[\"Decision Tree\",mse,msle,rmse,rmsle]],columns=[\"Model\",\"MSE\",\"MSLE\",\"RMSE\",\"RMSLE\"])\n",
    "results5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling Using RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance for the Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data on the model\n",
    "model_rf = rf_model.fit(Xm_train, ym_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "rf_pred = model_rf.predict(Xm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualise our actual and predicted values\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(ym_test,label=\"Actual sales\")\n",
    "plt.plot(rf_pred,label=\"Predicted\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Random Forest Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate our model\n",
    "mse=mean_squared_error(ym_test,rf_pred)\n",
    "msle=mean_squared_log_error(ym_test,rf_pred)\n",
    "rmse=np.sqrt(mean_squared_error(ym_test,rf_pred)).round(2)\n",
    "rmsle=np.sqrt(mean_squared_log_error(ym_test,rf_pred)).round(2)\n",
    "\n",
    "results6=pd.DataFrame([[\"Random Forest\",mse,msle,rmse,rmsle]],columns=[\"Model\",\"MSE\",\"MSLE\",\"RMSE\",\"RMSLE\"])\n",
    "results6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the results DataFrames into a single table\n",
    "combined = pd.concat([results, results1, results2,results3,results4,results5,results6])\n",
    "\n",
    "# Reset the index\n",
    "combined = combined.reset_index(drop=True)\n",
    "\n",
    "# Sort the combined results table\n",
    "combined = combined.sort_values(by=\"RMSLE\")\n",
    "\n",
    "# Display the combined results table\n",
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After comparing our univariate models to our multivariate models,decision tree has the lowest Root mean Square Log error(RMSLE) of 0.11 making it the best model as compared to the others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhancing Model Performance\n",
    "\n",
    "In this section, we will explore methods to enhance the model's performance through feature selection and evaluating feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance For Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our features from our train data\n",
    "feature_names = Xm_train.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use our model to get the important feature\n",
    "feature_importance = model_lin.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view feature importance\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we calculate the absolute value of the fearure importance and sort in descending order\n",
    "absolute_importance = np.abs(feature_importance)\n",
    "sorted_importance = sorted(absolute_importance, reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to hold feature names and their corresponding importance\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': sorted_importance})\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart\n",
    "fig = px.bar(importance_df, x='Feature', y='Importance', title='Feature Importance in Linear Regression Model')\n",
    "fig.update_xaxes(categoryorder='total descending')  # Sort x-axis by importance\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for graph to show on github\n",
    "\n",
    "# Create a DataFrame to hold feature names and their corresponding importance\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': sorted_importance})\n",
    "\n",
    "# Create a horizontal bar plot using Seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df, orient='h')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance in Linear Regression Model')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our graph we will drop quarter,is_weekend and day_of_month since they had the least importance in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dropping our unwanted features\n",
    "lin_X_train= Xm_train.drop( [\"quarter\",\"is_weekend\",\"day_of_month\"], axis= 1)\n",
    "\n",
    "lin_X_test= Xm_test.drop([\"quarter\",\"is_weekend\",\"day_of_month\"], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit our model \n",
    "lin_reg=lin_model.fit(lin_X_train, ym_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we predict\n",
    "lin_reg_pred=lin_reg.predict(lin_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate our model\n",
    "mse=mean_squared_error(ym_test,lin_reg_pred)\n",
    "msle=mean_squared_log_error(ym_test,lin_reg_pred)\n",
    "rmse=np.sqrt(mean_squared_error(ym_test,lin_reg_pred)).round(2)\n",
    "rmsle=np.sqrt(mean_squared_log_error(ym_test,lin_reg_pred)).round(2)\n",
    "\n",
    "results7=pd.DataFrame([[\"FS Linear Regression\",mse,msle,rmse,rmsle]],columns=[\"Model\",\"MSE\",\"MSLE\",\"RMSE\",\"RMSLE\"])\n",
    "results7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance For Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "feature_importance1 = model_dtree.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a DataFrame to store feature names and their importances\n",
    "importance_df1 = pd.DataFrame({'Feature': Xm_train.columns, 'Importance': feature_importance1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by importance in descending order\n",
    "importance_df1 = importance_df1.sort_values(by='Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view important features\n",
    "importance_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart using Plotly\n",
    "fig = px.bar(importance_df1, x='Feature', y='Importance', title='Feature Importances in Decision Tree Model')\n",
    "\n",
    "# Customize the layout (optional)\n",
    "fig.update_layout(xaxis_title='Feature', yaxis_title='Importance')\n",
    "fig.update_xaxes(categoryorder='total descending')  # Sort x-axis by importance\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot using Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df1, orient='h')\n",
    "plt.title('Feature Importances in Decision Tree Model')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our graph these columns will be dropped namely:\n",
    "\n",
    "- holiday_0\n",
    "- quarter\n",
    "- is_weekend\n",
    "- holiday_1\n",
    "- holiday_2\n",
    "- transferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dropping our unwanted features\n",
    "tree_X_train= Xm_train.drop([\"quarter\",\"is_weekend\",\"transferred\",\"holiday_0\",\"holiday_1\",\"holiday_2\"], axis= 1)\n",
    "\n",
    "tree_X_test= Xm_test.drop([\"quarter\",\"is_weekend\",\"transferred\",\"holiday_0\",\"holiday_1\",\"holiday_2\"], axis= 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit our model \n",
    "tree_m=model_dtree.fit(tree_X_train, ym_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we predict\n",
    "tree_p=tree_m.predict(tree_X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate our model\n",
    "mse=mean_squared_error(ym_test,tree_p)\n",
    "msle=mean_squared_log_error(ym_test,tree_p)\n",
    "rmse=np.sqrt(mean_squared_error(ym_test,tree_p)).round(2)\n",
    "rmsle=np.sqrt(mean_squared_log_error(ym_test,tree_p)).round(2)\n",
    "\n",
    "results8=pd.DataFrame([[\"FS Decision Tree\",mse,msle,rmse,rmsle]],columns=[\"Model\",\"MSE\",\"MSLE\",\"RMSE\",\"RMSLE\"])\n",
    "results8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection For Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "feature_importance2= rf_model.feature_importances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a DataFrame to store feature names and their importances\n",
    "importance_df2 = pd.DataFrame({'Feature': Xm_train.columns, 'Importance': feature_importance2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by importance in descending order\n",
    "importance_df2 = importance_df2.sort_values(by='Importance', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view important features\n",
    "importance_df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart using Plotly\n",
    "fig = px.bar(importance_df2, x='Feature', y='Importance', title='Feature Importances in Random Forest Model')\n",
    "\n",
    "# Customize the layout \n",
    "fig.update_layout(xaxis_title='Feature', yaxis_title='Importance')\n",
    "fig.update_xaxes(categoryorder='total descending')  # Sort x-axis by importance\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot using Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df2, orient='h')\n",
    "plt.title('Feature Importances in Random Forest Model')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our graph these columns will be dropped namely:\n",
    "\n",
    "- holiday_0\n",
    "- quarter\n",
    "- locale\n",
    "- holiday_1\n",
    "- holiday_2\n",
    "- transferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dropping our unwanted features\n",
    "forest_X_train= Xm_train.drop([\"quarter\",\"locale\",\"transferred\",\"holiday_0\",\"holiday_1\",\"holiday_2\"], axis= 1)\n",
    "\n",
    "forest_X_test= Xm_test.drop([\"quarter\",\"locale\",\"transferred\",\"holiday_0\",\"holiday_1\",\"holiday_2\"], axis= 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit our model \n",
    "forest_m=rf_model.fit(forest_X_train, ym_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we predict\n",
    "forest_pred=forest_m.predict(forest_X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate our model\n",
    "mse=mean_squared_error(ym_test,forest_pred)\n",
    "msle=mean_squared_log_error(ym_test,forest_pred)\n",
    "rmse=np.sqrt(mean_squared_error(ym_test,forest_pred)).round(2)\n",
    "rmsle=np.sqrt(mean_squared_log_error(ym_test,forest_pred)).round(2)\n",
    "\n",
    "results9=pd.DataFrame([[\"FS Random Forest\",mse,msle,rmse,rmsle]],columns=[\"Model\",\"MSE\",\"MSLE\",\"RMSE\",\"RMSLE\"])\n",
    "results9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Result Of All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe for all the models and sort\n",
    "final_result= pd.concat([results, results1,results2,results3,results4,results5,results6,results7,results8,results9])\n",
    "final_result = final_result.sort_values(by= \"RMSLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view model perfromances\n",
    "final_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
